{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WSmUMfM89xBy"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "p3U_xlmcSJDt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Dataset Selector ==================\n",
        "def load_dataset(name):\n",
        "    if name == 'pavia':\n",
        "        data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "        gt = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    elif name == 'salinas':\n",
        "        data = sio.loadmat('/content/Salinas_corrected.mat')['salinas_corrected']\n",
        "        gt = sio.loadmat('/content/Salinas_gt.mat')['salinas_gt']\n",
        "    elif name == 'indian':\n",
        "        data = sio.loadmat('/content/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "        gt = sio.loadmat('/content/Indian_pines_gt.mat')['indian_pines_gt']\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset name\")\n",
        "    return data, gt"
      ],
      "metadata": {
        "id": "V9VGIdry-aXe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Preprocessing ==================\n",
        "def preprocess(data, gt, dataset_name):\n",
        "    h, w, bands = data.shape\n",
        "    if dataset_name == 'indian':\n",
        "       noisy_bands = [b for b in (list(range(104, 109)) + list(range(150, 164)) + [220]) if b < data.shape[-1]]\n",
        "       data = np.delete(data, noisy_bands, axis=2)\n",
        "    scaler = MinMaxScaler()\n",
        "    data_reshaped = data.reshape(-1, data.shape[2])\n",
        "    data_scaled = scaler.fit_transform(data_reshaped)\n",
        "    pca_components = 30 if dataset_name != 'indian' else 40\n",
        "    pca = PCA(n_components=pca_components)\n",
        "    data_pca = pca.fit_transform(data_scaled)\n",
        "    data_pca = data_pca.reshape(h, w, -1)\n",
        "    return data_pca, gt, h, w, pca_components"
      ],
      "metadata": {
        "id": "VXkZeMJD-pjm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Patch Extraction ==================\n",
        "def extract_patches(data, gt, patch_size):\n",
        "    h, w, _ = data.shape\n",
        "    margin = patch_size // 2\n",
        "    padded_data = np.pad(data, ((margin, margin), (margin, margin), (0, 0)), mode='reflect')\n",
        "    padded_gt = np.pad(gt, ((margin, margin), (margin, margin)), mode='reflect')\n",
        "    patches, labels, coords = [], [], []\n",
        "    for i in range(margin, margin + h):\n",
        "        for j in range(margin, margin + w):\n",
        "            patch = padded_data[i - margin:i + margin, j - margin:j + margin, :]\n",
        "            label = padded_gt[i, j]\n",
        "            if label != 0:\n",
        "                patches.append(patch)\n",
        "                labels.append(label)\n",
        "                coords.append((i - margin, j - margin))\n",
        "    return np.array(patches), np.array(labels), np.array(coords), h, w"
      ],
      "metadata": {
        "id": "kdvh14-y-x_I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Autoencoder ==================\n",
        "class PatchAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out, z"
      ],
      "metadata": {
        "id": "p2mNzuyq-05h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, dim=32, heads=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z should be [batch_size, latent_dim] â†’ [batch_size, 1, latent_dim]\n",
        "        z = z.unsqueeze(1)\n",
        "        if z.shape[-1] != self.attn.embed_dim:\n",
        "            print(f\"[ERROR] Input dim mismatch: z.shape = {z.shape}, expected embed_dim = {self.attn.embed_dim}\")\n",
        "            raise ValueError(\"Latent dim mismatch with transformer input.\")\n",
        "\n",
        "        attn_out, _ = self.attn(z, z, z)\n",
        "        # Should be [batch_size, 1, dim], squeeze to [batch_size, dim]\n",
        "        squeezed = attn_out.squeeze(1)\n",
        "        if squeezed.shape[-1] != self.linear[0].in_features:\n",
        "            print(f\"[ERROR] Linear layer input mismatch: {squeezed.shape}\")\n",
        "            raise ValueError(\"Transformer output mismatch with linear layer.\")\n",
        "\n",
        "        scores = self.linear(squeezed).squeeze()\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "1wYmer3l-4PZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ================== Dynamic Pipeline ==================\n",
        "# def run_pipeline(dataset_name, patch_size=24, latent_dim=32):\n",
        "#     torch.cuda.empty_cache()  # Free GPU memory\n",
        "\n",
        "#     data, gt = load_dataset(dataset_name)\n",
        "#     data_pca, gt, h, w, pca_dim = preprocess(data, gt, dataset_name)  # h, w, pca_dim returned here\n",
        "#     input_dim = patch_size * patch_size * pca_dim\n",
        "\n",
        "#     patches, labels, coords, h, w = extract_patches(data_pca, gt, patch_size=patch_size)\n",
        "\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     model = PatchAutoencoder(latent_dim=latent_dim, input_dim=input_dim).to(device)\n",
        "#     patches_tensor = torch.tensor(patches).permute(0, 3, 1, 2).float().to(device)\n",
        "#     patches_tensor = patches_tensor.reshape(-1, input_dim)\n",
        "\n",
        "#     criterion = nn.MSELoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#     print(\"Training Autoencoder:\")\n",
        "#     for epoch in range(5):  # Reduce epochs for memory safety\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output, _ = model(patches_tensor)\n",
        "#         loss = criterion(output, patches_tensor)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch+1}/5, Training Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         _, latent_z = model(patches_tensor)\n",
        "\n",
        "#     transformer = SimpleTransformer(dim=latent_dim).to(device)\n",
        "#     trans_scores = []\n",
        "#     batch_size = 1024\n",
        "#     for i in range(0, latent_z.shape[0], batch_size):\n",
        "#         batch = latent_z[i:i+batch_size]\n",
        "#         with torch.no_grad():\n",
        "#             scores = transformer(batch.to(device)).cpu().numpy()\n",
        "#         trans_scores.extend(scores)\n",
        "#     trans_scores = np.array(trans_scores)"
      ],
      "metadata": {
        "id": "wBZeQgjIGJ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualize Anomaly Scores on Ground Truth\n",
        "# anomaly_map = np.zeros((h, w))\n",
        "# for idx, (x, y) in enumerate(coords):\n",
        "#     anomaly_map[x, y] = trans_scores[idx]\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.subplot(1, 3, 1)\n",
        "# plt.title(\"Ground Truth\")\n",
        "# plt.imshow(gt, cmap='gray')\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.subplot(1, 3, 2)\n",
        "# plt.title(\"Anomaly Heatmap\")\n",
        "# plt.imshow(anomaly_map, cmap='hot')\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.subplot(1, 3, 3)\n",
        "# plt.title(\"Overlay\")\n",
        "# plt.imshow(gt, cmap='gray')\n",
        "# plt.imshow(anomaly_map, cmap='hot', alpha=0.5)\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(latent_z.cpu().numpy(), labels, test_size=0.3, random_state=42)\n",
        "# svm = SVC(kernel='rbf', class_weight='balanced')\n",
        "# svm.fit(X_train, y_train)\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# print(\"SVM Classification Report:\")\n",
        "# print(classification_report(y_test, y_pred))\n",
        "# conf = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(conf, annot=True, fmt='d', cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.xlabel(\"Predicted\")\n",
        "# plt.ylabel(\"True\")\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(trans_scores, bins=100, kde=True, color='orange')\n",
        "# plt.title(f\"Transformer Anomaly Score Distribution - {dataset_name.upper()}\")\n",
        "# plt.xlabel(\"Anomaly Score\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.grid()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "V1sd0GeJ-8sd",
        "outputId": "abb891e0-0b49-43e9-98d8-b1c20d5c9f2a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'h' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b717ee834021>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize Anomaly Scores on Ground Truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manomaly_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0manomaly_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Run Pipeline ==================\n",
        "def run_pipeline(dataset_name, patch_size=16, latent_dim=32, num_epochs=10):\n",
        "    os.makedirs(\"outputs\", exist_ok=True)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Loading dataset and preprocessing...\")\n",
        "    data, gt = load_dataset(dataset_name)\n",
        "    data_pca, gt, h, w, pca_dim = preprocess(data, gt, dataset_name)\n",
        "    input_dim = patch_size * patch_size * pca_dim\n",
        "    patches, labels, coords, h, w = extract_patches(data_pca, gt, patch_size=patch_size)\n",
        "\n",
        "    print(\"Initializing Autoencoder...\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = PatchAutoencoder(latent_dim=latent_dim, input_dim=input_dim).to(device)\n",
        "\n",
        "    patches_tensor = torch.tensor(patches).permute(0, 3, 1, 2).float()\n",
        "    patches_tensor = patches_tensor.reshape(-1, input_dim)\n",
        "    train_loader = DataLoader(TensorDataset(patches_tensor), batch_size=256, shuffle=True)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    epoch_losses = []\n",
        "    print(\"Training Autoencoder:\")\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch[0].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(batch)\n",
        "            loss = criterion(output, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, num_epochs+1), epoch_losses, marker='o')\n",
        "    plt.title(\"Autoencoder Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid()\n",
        "    plt.savefig(f\"outputs/{dataset_name}_ae_loss_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Extracting latent features...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, latent_z = model(patches_tensor.to(device))\n",
        "\n",
        "    print(\"Visualizing Latent Space...\")\n",
        "    visualize_latent_space(latent_z.cpu().numpy(), labels, dataset_name)\n",
        "\n",
        "    print(\"Running Transformer on latent features...\")\n",
        "    transformer = SimpleTransformer(dim=latent_dim).to(device)\n",
        "    trans_scores = []\n",
        "    batch_size = 256\n",
        "    for i in range(0, latent_z.shape[0], batch_size):\n",
        "        batch = latent_z[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "            scores = transformer(batch.to(device)).cpu().numpy()\n",
        "        trans_scores.extend(scores)\n",
        "    trans_scores = np.array(trans_scores)\n",
        "\n",
        "    print(\"Generating Anomaly Map...\")\n",
        "    anomaly_map = np.zeros((h, w))\n",
        "    for idx, (x, y) in enumerate(coords):\n",
        "        anomaly_map[x, y] = trans_scores[idx]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.imshow(gt, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Anomaly Heatmap\")\n",
        "    plt.imshow(anomaly_map, cmap='hot')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Overlay\")\n",
        "    plt.imshow(gt, cmap='gray')\n",
        "    plt.imshow(anomaly_map, cmap='hot', alpha=0.5)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"outputs/{dataset_name}_anomaly_map.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Training SVM Classifier on Latent Features...\")\n",
        "    X = latent_z.cpu().numpy()\n",
        "    y = np.array(labels)\n",
        "\n",
        "\n",
        "    # Sanity check for NaN/Inf\n",
        "    if np.isnan(X).any() or np.isinf(X).any():\n",
        "        print(\"[WARNING] Found NaNs or Infs in latent features. Cleaning up...\")\n",
        "        X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6) # Correctly call np.nan_to_num\n",
        "\n",
        "    # Optional: remove duplicate entries if any\n",
        "    #unique_rows = ~np.all(np.isclose(X, 0.0), axis=1) #This line was incomplete and causing the original error\n",
        "    #X = X[unique_rows]\n",
        "    #y = y[unique_rows]\n",
        "\n",
        "    # SVM Training\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "        svm = SVC(kernel='rbf', class_weight='balanced')\n",
        "        svm.fit(X_train, y_train)\n",
        "        y_pred = svm.predict(X_test)\n",
        "\n",
        "        print(\"SVM Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        conf = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure()\n",
        "        sns.heatmap(conf, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.savefig(f\"outputs/{dataset_name}_conf_matrix_dynamic.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] SVM training failed: {e}\")"
      ],
      "metadata": {
        "id": "AN5XY-4vGcnC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Visualizing Latent Space (t-SNE) ==================\n",
        "def visualize_latent_space(z, labels, dataset_name):\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    z_2d = tsne.fit_transform(z)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap='tab20', s=5)\n",
        "    plt.title(f\"Latent Space t-SNE Visualization - {dataset_name.upper()}\")\n",
        "    plt.colorbar(scatter)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"outputs/{dataset_name}_latent_tsne.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "wa4ujOhXDIbi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aq2Whd-HwrR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Run ==================\n",
        "run_pipeline('pavia')"
      ],
      "metadata": {
        "id": "3m-lTY3z-_8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48862913-9abb-4eee-c5f0-2b30ec1892e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset and preprocessing...\n",
            "Initializing Autoencoder...\n",
            "Training Autoencoder:\n",
            "Epoch 1/10 - Loss: 0.010470\n",
            "Epoch 2/10 - Loss: 0.004926\n",
            "Epoch 3/10 - Loss: 0.004067\n",
            "Epoch 4/10 - Loss: 0.003738\n",
            "Epoch 5/10 - Loss: 0.003597\n",
            "Epoch 6/10 - Loss: 0.003503\n",
            "Epoch 7/10 - Loss: 0.003431\n",
            "Epoch 8/10 - Loss: 0.003362\n",
            "Epoch 9/10 - Loss: 0.003331\n",
            "Epoch 10/10 - Loss: 0.003284\n",
            "Extracting latent features...\n",
            "Visualizing Latent Space...\n",
            "Running Transformer on latent features...\n",
            "Generating Anomaly Map...\n",
            "Training SVM Classifier on Latent Features...\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.93      0.96      1994\n",
            "           2       0.98      0.86      0.91      5617\n",
            "           3       0.89      0.97      0.93       648\n",
            "           4       1.00      0.99      0.99       894\n",
            "           5       1.00      1.00      1.00       414\n",
            "           6       0.64      0.94      0.76      1508\n",
            "           7       0.82      0.99      0.90       395\n",
            "           8       0.99      0.98      0.98      1114\n",
            "           9       1.00      1.00      1.00       249\n",
            "\n",
            "    accuracy                           0.92     12833\n",
            "   macro avg       0.92      0.96      0.94     12833\n",
            "weighted avg       0.94      0.92      0.92     12833\n",
            "\n"
          ]
        }
      ]
    }
  ]
}